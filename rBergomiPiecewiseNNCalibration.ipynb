{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Surface Calibration - rBergomi Piecewise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myscale(x,ub,lb):\n",
    "    res = np.zeros(len(ub))\n",
    "    for i in range(len(ub)):\n",
    "        res[i] = (x[i] - (ub[i] + lb[i]) * 0.5) * 2 / (ub[i] - lb[i])\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def myinverse(x,ub,lb):\n",
    "    res = np.zeros (len(ub))\n",
    "    for i in range (len(ub)):\n",
    "        res[i] = x[i] * (ub[i] - lb[i]) * 0.5 + (ub[i] + lb[i]) * 0.5\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtransform(X_train, X_test):\n",
    "    return [scale2.transform (X_train), scale2.transform (X_test)]\n",
    "\n",
    "\n",
    "def xinversetransform(x):\n",
    "    return scale2.inverse_transform (x)\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementation of a NN in numpy, for a faster computation during calibration\n",
    "- Cost functions and relative Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumLayers=4\n",
    "\n",
    "def elu(x):\n",
    "    #Careful function ovewrites x\n",
    "    ind=(x<0)\n",
    "    x[ind]=np.exp(x[ind])-1\n",
    "    return x\n",
    "\n",
    "def eluPrime(y):\n",
    "    # we make a deep copy of input x\n",
    "    x=np.copy(y)\n",
    "    ind=(x<0)\n",
    "    x[ind]=np.exp(x[ind])\n",
    "    x[~ind]=1\n",
    "    return x\n",
    "\n",
    "def NeuralNetwork(x):\n",
    "    input1=x\n",
    "    for i in range(NumLayers):\n",
    "        input1=input1@NNParameters[i][0]+NNParameters[i][1]\n",
    "        #Elu activation\n",
    "        input1=elu(input1)\n",
    "    #The output layer is linear\n",
    "    i+=1\n",
    "    \n",
    "    return input1@NNParameters[i][0]+NNParameters[i][1]\n",
    "\n",
    "def NeuralNetworkGradient(x):\n",
    "    input1=x\n",
    "    #Identity Matrix represents Jacobian with respect to initial parameters\n",
    "    grad=np.eye(len(x))\n",
    "    #Propagate the gradient via chain rule\n",
    "    for i in range(NumLayers):\n",
    "        input1=input1@NNParameters[i][0]+NNParameters[i][1]\n",
    "        grad=grad@NNParameters[i][0]\n",
    "        #Elu activation\n",
    "        grad*=eluPrime(input1)\n",
    "        input1=elu(input1)\n",
    "    #input1.append(np.dot(input1[i],NNParameters[i+1][0])+NNParameters[i+1][1])\n",
    "    grad= grad@NNParameters[i+1][0]\n",
    "    #grad stores all intermediate Jacobians, however only the last one is used here as output\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CostFunc(x,sample_ind):\n",
    "    return np.sum(np.power((NeuralNetwork(x)-x_test_transform[sample_ind]),2))\n",
    "def Jacobian(x,sample_ind):\n",
    "    return 2*np.sum((NeuralNetwork(x)-x_test_transform[sample_ind])*NeuralNetworkGradient(x),axis=1)\n",
    "#Cost Function for Levenberg Marquardt\n",
    "def CostFuncLS(x,sample_ind):\n",
    "    return (NeuralNetwork(x)-x_test_transform[sample_ind])\n",
    "def JacobianLS(x,sample_ind):\n",
    "    return NeuralNetworkGradient(x).T\n",
    "def CostFuncLS_LM(x,sample_ind):\n",
    "    return (NeuralNetwork(x)-xx_transform[sample_ind])\n",
    "def JacobianLS_LM(x,sample_ind):\n",
    "    return NeuralNetworkGradient(x).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial setting of variables of interest and model specification. Also, loading the dataset and performing some pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import scipy\n",
    "\n",
    "model_name = 'rBergomiTermStructure'\n",
    "num_params = 11\n",
    "\n",
    "strikes = np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5])\n",
    "maturities = np.array([0.1, 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.0])\n",
    "\n",
    "S0=1.\n",
    "strikes_dim=len(strikes)\n",
    "maturities_dim=len(maturities)\n",
    "\n",
    "f = gzip.GzipFile('Dataset/' + model_name + 'TrainSet.txt.gz', \"r\")\n",
    "dat = np.load(f)\n",
    "f.close()\n",
    "xx = dat[:, :num_params]\n",
    "yy = dat[:, num_params:]\n",
    "\n",
    "data_len = len(xx)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(yy, xx, test_size=0.15, random_state=42)\n",
    "\n",
    "ub = np.max(xx,axis=0)\n",
    "lb = np.min(xx,axis=0)\n",
    "\n",
    "scale2 = StandardScaler()\n",
    "scale2.fit(X_train)\n",
    "\n",
    "[x_train_transform, x_test_transform] = [scale2.transform (X_train), scale2.transform (X_test)]\n",
    "\n",
    "xx_transform = scale2.transform(yy)\n",
    "\n",
    "y_train_transform = np.array([myscale(y,ub,lb) for y in y_train])\n",
    "y_test_transform = np.array([myscale(y,ub,lb) for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(scale2, open('Scales/'+model_name+'Scaler.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximation of the mapping between parameters and volatility surface:\n",
    "$$\\theta^{params} \\mapsto Vol_{surface}$$\n",
    "\n",
    "The function is implemented to be completely working as it is, for this reason there will be a second round of data loading and pre-processing. The code is actually fully run only if no pre-existing model weight are found. This in order to avoid usless and costly code running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneralizedNNCalibration(model_name,n_parameters,strikes,maturities,random_state=42,plot_history=False,verbose=-1):\n",
    "    \n",
    "    f = gzip.GzipFile('Dataset/' + model_name + 'TrainSet.txt.gz', \"r\")\n",
    "    dat = np.load(f)\n",
    "    f.close()\n",
    "    xx = dat[:, :n_parameters]\n",
    "    yy = dat[:, n_parameters:]\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(yy, xx, test_size=0.15, random_state=42)\n",
    "\n",
    "    ub = np.max(xx,axis=0)\n",
    "    lb = np.min(xx,axis=0)\n",
    "    \n",
    "    scale2 = StandardScaler()\n",
    "    scale2.fit(X_train)\n",
    "\n",
    "    [x_train_transform, x_test_transform] = [scale2.transform (X_train), scale2.transform (X_test)]\n",
    "\n",
    "    y_train_transform = np.array([myscale(y,ub,lb) for y in y_train])\n",
    "    y_test_transform = np.array([myscale(y,ub,lb) for y in y_test])\n",
    "    \n",
    "    # %% Construct the NN\n",
    "    \n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Sequential\n",
    "    from keras.callbacks import EarlyStopping\n",
    "\n",
    "    #'''\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(32,activation = 'elu',input_shape=(n_parameters,)))\n",
    "    model.add(Dense(32,activation = 'elu'))\n",
    "    model.add(Dense(32,activation = 'elu'))\n",
    "    model.add(Dense(32,activation = 'elu'))\n",
    "\n",
    "    model.add(Dense(88,activation = 'linear'))\n",
    "\n",
    "    try:\n",
    "        model.load_weights('ModelWeights/' + model_name + 'NNWeights.h5')\n",
    "    \n",
    "    except:\n",
    "        model.compile(loss = root_mean_squared_error, optimizer = \"adam\")\n",
    "\n",
    "        earlystop = EarlyStopping(monitor='val_loss', mode='min', patience=150, restore_best_weights=True)\n",
    "        \n",
    "        model_history = model.fit(y_train_transform, x_train_transform, batch_size=128,\\\n",
    "                        validation_split = 0.1, epochs = 1000, \\\n",
    "                                  verbose = -1,shuffle=1,callbacks=[earlystop])\n",
    "        \n",
    "        if plot_history:\n",
    "            plt.figure(1,figsize=(14,4))\n",
    "            fig, axs = plt.subplots(1, 2)\n",
    "            axs[0].plot(model_history.history['loss'])\n",
    "            axs[0].set_title('MSE')\n",
    "            axs[1].plot(model_history.history['val_loss'], 'tab:orange')\n",
    "            axs[1].set_title('validation MSE')\n",
    "\n",
    "            for ax in axs.flat:\n",
    "                ax.set(xlabel='Epochs')\n",
    "\n",
    "            # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "            for ax in axs.flat:\n",
    "                ax.label_outer()\n",
    "            plt.savefig('CreatedImages/'+model_name+'PerformanceHistory.png', dpi=300)\n",
    "            \n",
    "        model.save_weights('ModelWeights/' + model_name + 'NNWeights.h5')\n",
    "\n",
    "    NNParameters=[]\n",
    "    for i in range(len(model.layers)):\n",
    "        NNParameters.append(model.layers[i].get_weights())\n",
    "\n",
    "    return (model,NNParameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the degree of approximation supported by a graphic representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def heat_maps(model):\n",
    "    \n",
    "    strikeslabel=np.round(np.linspace(strikes[0],strikes[-1],8),1)\n",
    "    maturitieslabel=np.array([0.1,0.2, 0.6, 1.5,1.8])\n",
    "    ##### AVERAGE VALUES #######\n",
    "    X_sample = xinversetransform(x_test_transform)\n",
    "    y_sample = y_test_transform\n",
    "\n",
    "    print (\"Computing Prediction...\",end=\"\\r\")\n",
    "    \n",
    "    prediction=[xinversetransform(NeuralNetwork(y_sample[i])) for i in range(len(y_sample))]\n",
    "    \n",
    "    print (\"Average Relative Error...\",end=\"\\r\")\n",
    "    \n",
    "    plt.figure(1,figsize=(14,4))\n",
    "    ax=plt.subplot(1,3,1)\n",
    "    err = np.mean(100*np.abs((X_sample-prediction)/X_sample),axis = 0)\n",
    "    plt.title(\"Average relative error\",fontsize=15,y=1.04)\n",
    "    plt.imshow(err.reshape(maturities_dim,strikes_dim))\n",
    "    plt.colorbar(format=mtick.PercentFormatter())\n",
    "    ax.set_xticks(np.linspace(0,len(strikes)-1,len(strikes)))\n",
    "    ax.set_xticklabels(strikes)\n",
    "    ax.set_yticks(np.linspace(0,len(maturities)-1,len(maturities)))\n",
    "    ax.set_yticklabels(maturities)\n",
    "    plt.xlabel(\"Strike\",fontsize=15,labelpad=5)\n",
    "    plt.ylabel(\"Maturity\",fontsize=15,labelpad=5)\n",
    "\n",
    "    print (\"Std Relative Error...\",end=\"\\r\")\n",
    "    \n",
    "    ax=plt.subplot(1,3,2)\n",
    "    err = 100*np.std(np.abs((X_sample-prediction)/X_sample),axis = 0)\n",
    "    plt.title(\"Std relative error\",fontsize=15,y=1.04)\n",
    "    plt.imshow(err.reshape(maturities_dim,strikes_dim))\n",
    "    plt.colorbar(format=mtick.PercentFormatter())\n",
    "    ax.set_xticks(np.linspace(0,len(strikes)-1,len(strikes)))\n",
    "    ax.set_xticklabels(strikes)\n",
    "    ax.set_yticks(np.linspace(0,len(maturities)-1,len(maturities)))\n",
    "    ax.set_yticklabels(maturities)\n",
    "    plt.xlabel(\"Strike\",fontsize=15,labelpad=5)\n",
    "    plt.ylabel(\"Maturity\",fontsize=15,labelpad=5)\n",
    "\n",
    "    print (\"Maximum Relative Error...\",end=\"\\r\")\n",
    "    \n",
    "    ax=plt.subplot(1,3,3)\n",
    "    err = 100*np.max(np.abs((X_sample-prediction)/X_sample),axis = 0)\n",
    "    plt.title(\"Maximum relative error\",fontsize=15,y=1.04)\n",
    "    plt.imshow(err.reshape(maturities_dim,strikes_dim))\n",
    "    plt.colorbar(format=mtick.PercentFormatter())\n",
    "    ax.set_xticks(np.linspace(0,len(strikes)-1,len(strikes)))\n",
    "    ax.set_xticklabels(strikes)\n",
    "    ax.set_yticks(np.linspace(0,len(maturities)-1,len(maturities)))\n",
    "    ax.set_yticklabels(maturities)\n",
    "    plt.xlabel(\"Strike\",fontsize=15,labelpad=5)\n",
    "    plt.ylabel(\"Maturity\",fontsize=15,labelpad=5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"CreatedImages/\"+model_name+'NNErrors.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot sample Smiles\n",
    "\n",
    "def plotCurves(model,sample_ind=3000):\n",
    "\n",
    "    X_sample = X_test[sample_ind]\n",
    "    y_sample = y_test_transform[sample_ind]\n",
    "    #print(scale.inverse_transform(y_sample))\n",
    "\n",
    "    prediction=xinversetransform(NeuralNetwork(y_sample))\n",
    "    plt.figure(1,figsize=(14,6))\n",
    "    for i in range(maturities_dim):\n",
    "        plt.subplot(2,4,i+1)\n",
    "\n",
    "        plt.plot(np.log(S0/strikes),X_sample[i*strikes_dim:(i+1)*strikes_dim],'b',label=\"Input data\")\n",
    "        plt.plot(np.log(S0/strikes),prediction[i*strikes_dim:(i+1)*strikes_dim],'--r',label=\"NN Approx\")\n",
    "\n",
    "\n",
    "        plt.title(\"Maturity=%1.2f \"%maturities[i])\n",
    "        plt.xlabel(\"log-moneyness\")\n",
    "        plt.ylabel(\"Implied vol\")\n",
    "\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"CreatedImages/\"+model_name+'CurveApprox.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(model_1,NNParameters) = GeneralizedNNCalibration(model_name,num_params,strikes,maturities, 42, plot_history=True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"NNParams/\"+model_name+\"NNParams\",NNParameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing the two methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit NeuralNetwork(y_test_transform[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit model_1.predict(y_test_transform[0].reshape(1,num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And running graphic anlysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "idx = 2000\n",
    "X_predict = xinversetransform(NeuralNetwork(y_test_transform))\n",
    "X_sample =  xinversetransform(x_test_transform)\n",
    "\n",
    "\n",
    "vola_surf_1 = X_sample[idx]\n",
    "vola_surf_2 = X_predict[idx]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"},figsize=(10,10))\n",
    "\n",
    "X_maturities, Y_strikes = np.meshgrid(strikes,maturities)\n",
    "ax.view_init(30,20)\n",
    "surf = ax.plot_surface(X_maturities, Y_strikes,vola_surf_1.reshape(maturities_dim,strikes_dim),linewidth=0,alpha = 0.9,cmap=cm.coolwarm, antialiased=True)\n",
    "ax.scatter(X_maturities, Y_strikes,vola_surf_2.reshape(maturities_dim,strikes_dim),color=\"k\",alpha=1)#,linewidth=10,alpha = 1,cmap=cm.winter, antialiased=True)\n",
    "\n",
    "ax.set_xlabel(\"Strike\",fontsize=15,labelpad=5)\n",
    "ax.set_ylabel(\"Maturity\",fontsize=15,labelpad=5)\n",
    "ax.set_zlabel(\"Volatility\",fontsize=15,labelpad=5)\n",
    "\n",
    "colBar = fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "ax.set_title(\"Input data vs NN Approx\")\n",
    "\n",
    "plt.savefig(\"CreatedImages/\" + model_name + '_volaSurf.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_maps(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotCurves(model_1,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, actual volatility calibraiton is performed. 4 different methods are compared, in terms of computaitional time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy\n",
    "Approx=[]\n",
    "Timing=[]\n",
    "\n",
    "solutions=np.zeros([4,num_params])\n",
    "times=np.zeros(4)\n",
    "init=np.zeros(num_params)\n",
    "for i in range(len(X_test)):\n",
    "    disp=str(i+1)+\"/\"+str(len(X_test))\n",
    "    print (disp,end=\"\\r\")\n",
    "    #L-BFGS-B\n",
    "    \n",
    "    start= time.time()\n",
    "    I=scipy.optimize.minimize(CostFunc,x0=init,args=i,method='L-BFGS-B',jac=Jacobian,tol=1E-10,options={\"maxiter\":5000})\n",
    "    end= time.time()\n",
    "    solutions[0,:]=myinverse(I.x,ub,lb)\n",
    "    times[0]=end-start\n",
    "    #SLSQP\n",
    "    start= time.time()\n",
    "    I=scipy.optimize.minimize(CostFunc,x0=init,args=i,method='SLSQP',jac=Jacobian,tol=1E-10,options={\"maxiter\":5000})\n",
    "    end= time.time()\n",
    "    solutions[1,:]=myinverse(I.x,ub,lb)\n",
    "    times[1]=end-start\n",
    "    #BFGS\n",
    "    start= time.time()\n",
    "    I=scipy.optimize.minimize(CostFunc,x0=init,args=i,method='BFGS',jac=Jacobian,tol=1E-10,options={\"maxiter\":5000})\n",
    "    end= time.time()\n",
    "    solutions[2,:]=myinverse(I.x,ub,lb)\n",
    "    times[2]=end-start\n",
    "    \n",
    "    #Levenberg-Marquardt\n",
    "    start= time.time()\n",
    "    I=scipy.optimize.least_squares(CostFuncLS,init, JacobianLS,args=(i,),gtol=1E-10)\n",
    "    end= time.time()\n",
    "    solutions[3,:]=myinverse(I.x,ub,lb)\n",
    "    times[3]=end-start\n",
    "    #I=scipy.optimize.minimize(costfunc,x0=init,method='COBYLA',options={\"maxiter\":2000})\n",
    "    #print(I)\n",
    "    Approx.append(np.copy(solutions))\n",
    "    Timing.append(np.copy(times))\n",
    "    #print(\"Approx \",scale.inverse_transform(I.x))\n",
    "    #print(\"True: \",scale.inverse_transform(y_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods=[\"L-BFGS-B \",\"SLSQP\",\"BFGS\",\"Levenberg-Marquardt\"]\n",
    "plt.figure(1,figsize=(12,6))\n",
    "plt.bar(methods,np.mean(Timing,axis=0)*1000, color = 'seagreen')\n",
    "plt.title(\"Gradient Method Average Calibration Time\",fontsize=25)\n",
    "plt.ylabel(\"Miliseconds\",fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.savefig('AverageCalibrationTime.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ind = len(X_test)\n",
    "X_sample = X_test[-1]\n",
    "y_sample = y_test_transform[sample_ind-1]\n",
    "\n",
    "init=np.zeros(num_params)\n",
    "I=scipy.optimize.least_squares(CostFuncLS,init, JacobianLS,args=(sample_ind-1,),gtol=1E-10)\n",
    "\n",
    "print(\"Approx \",myinverse(I.x,ub,lb))\n",
    "print(\"True: \",myinverse(y_sample,ub,lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing calibration error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMParameters = []\n",
    "init=np.zeros(num_params)\n",
    "for i in range(len(X_test)):\n",
    "    disp=str(i+1)+\"/\"+str(len(X_test))\n",
    "    print (disp,end=\"\\r\")\n",
    "    \n",
    "    #Levenberg-Marquardt\n",
    "    I=scipy.optimize.least_squares(CostFuncLS,init, JacobianLS,args=(i,),gtol=1E-10)\n",
    "    LMParameters.append(myinverse(I.x,ub,lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=[\"$\\\\xi_1$\",\"$\\\\xi_2$\",\"$\\\\xi_3$\",\"$\\\\xi_4$\",\"$\\\\xi_5$\",\"$\\\\xi_6$\",\"$\\\\xi_7$\",\"$\\\\xi_8$\",\"$\\\\nu$\",\"$\\\\rho$\",\"$H$\"]\n",
    "average=np.zeros([num_params,len(X_test)])\n",
    "fig=plt.figure(figsize=(10,14))\n",
    "for u in range(num_params):\n",
    "    ax=plt.subplot(6,2,u+1)\n",
    "    for i in range(len(X_test)):\n",
    "        \n",
    "        y=y_test[i][u]\n",
    "        plt.plot(y,100*np.abs(LMParameters[i][u]-y)/np.abs(y),'b*')\n",
    "        average[u,i]=np.abs(LMParameters[i][u]-y)/np.abs(y)\n",
    "    plt.title(titles[u],fontsize=20)\n",
    "    plt.ylabel('relative Error',fontsize=15)\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter() )\n",
    "    plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=15)\n",
    "    plt.text(0.5, 0.8, 'Average: %1.2f%%\\n Median:   %1.2f%% '%(np.mean(100*average[u,:]),np.quantile(100*average[u,:],0.5)), horizontalalignment='center',verticalalignment='center', transform=ax.transAxes,fontsize=15)\n",
    "\n",
    "    print(\"average= \",np.mean(average[u,:]))\n",
    "plt.tight_layout()\n",
    "plt.savefig('CreatedImages/'+model_name+'ParameterRelativeErrors.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
